# awesome_LLMs_interview_notes
LLMs interview notes and answers

问题来自 [LLMs 千面郎君： km1994 - LLMs_interview_notes ](https://github.com/km1994/LLMs_interview_notes)

**答案 为 自己编写，不保证正确，仅供参考。**

- [x] **[大模型（LLMs）基础面](./大模型（LLMs）基础面.md)**
  - [x] 1. 目前 主流的开源模型体系 有哪些？
  - [x] 2. prefix LM 和 causal LM 区别是什么？
  - [x] 3. 涌现能力是啥原因？
  - [x] 4. 大模型LLM的架构介绍？
- [x] **[大模型（LLMs）进阶面](./大模型（LLMs）进阶面.md)**
  - [x] 1. LLMs 复读机问题
    - [x] 1. 什么是 LLMs 复读机问题？
    - [x] 2. 为什么会出现 LLMs 复读机问题？
    - [x] 3. 如何缓解 LLMs 复读机问题？
  - [x] 2. llama 系列问题
    - [x] 1. llama 输入句子长度理论上可以无限长吗？
  - [x] 3. 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选？
  - [x] 4. 各个专业领域是否需要各自的大模型来服务？
  - [x] 5. 如何让大模型处理更长的文本？
- [x] **[大模型（LLMs）微调面](./大模型（LLMs）微调面.md)**
- [x] **[大模型（LLMs）langchain面]()**
  - [x] **[大模型（LLMs）langchain 面](./大模型（LLMs）langchain面/大模型（LLMs）langchain面.md)**
  - [x] **[基于LLM+向量库的文档对话 经验面](./大模型（LLMs）langchain面/基于LLM+向量库的文档对话经验面.md)**
- [x] **[大模型（LLMs）参数高效微调(PEFT) 面]()**
  - [x] **[大模型（LLMs）参数高效微调(PEFT) 面](./大模型（LLMs）参数高效微调(PEFT)面/大模型（LLMs）参数高效微调(PEFT)面.md)**
  - [x] **[适配器微调（Adapter-tuning）篇](./大模型（LLMs）参数高效微调(PEFT)面/适配器微调（Adapter-tuning）篇.md)**
  - [x] **[提示学习（Prompting）](./大模型（LLMs）参数高效微调(PEFT)面/提示学习（Prompting）.md)**
  - [x] **[LoRA 系列篇](./大模型（LLMs）参数高效微调(PEFT)面/LoRA系列篇.md)**
- [x] **[大模型（LLMs）推理面](./大模型（LLMs）推理面.md)**
- [x] **[大模型（LLMs）评测面](./大模型（LLMs）评测面.md)**
- [x] **[大模型（LLMs）强化学习面](./大模型（LLMs）强化学习面.md)**
- [x] **[大模型（LLMs）软硬件配置面](./大模型（LLMs）软硬件配置面.md)**
- [x] **[大模型（LLMs）训练集面](./大模型（LLMs）训练集面.md)**
- [ ] **[大模型（LLMs）显存问题面](./大模型（LLMs）显存问题面.md)**
- [ ] **[大模型（LLMs）分布式训练面](./大模型（LLMs）分布式训练面.md)**
- [x] **[大模型（LLMs）agent 面](./大模型（LLMs）agent面.md)**



